# -*- coding: utf-8 -*-
import scrapy
import sys
from exploit_db.items import ExploitDbItem
from scrapy.selector import HtmlXPathSelector
from scrapy import Request


class ExploitdbSpider(scrapy.Spider):
    name = "exploitdb"
    allowed_domains = ["exploit-db.com"]
    start_urls = ['https://www.exploit-db.com/rss.xml']

    def start_requests(self):
        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:48.0) Gecko/20100101 Firefox/48.0'}
        for url in self.start_urls:
            yield Request(url, headers=headers)

    def parse(self, response):
        for page in response.xpath('//item/link/text()').extract():
            print "URL : ",page
            url = response.urljoin(page)
            if page is not None:
                headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:48.0) Gecko/20100101 Firefox/48.0'}
                yield scrapy.Request(url, callback=self.parse_inner_contents, headers=headers)

    def parse_inner_contents(self,response):
        item = ExploitDbItem()
        for sel in response.xpath('//div[@class="l-canvas sidebar_none type_wide"]'):
            item['title'] = sel.xpath('//h1[@itemprop="headline"]/text()').extract()
            item['desc'] = sel.xpath('//main//div[@id="container"]/pre/text()').extract()
            item['published'] = sel.xpath('//table[@class="exploit_list"]//tr[1]/td[3]//span//text()').extract()
            item['platform'] = sel.xpath('//tr[2]/td[3]/a/text()').extract()
            item['cve'] = sel.xpath('//tr[2]/td[1]/text()').extract()
            item['_type'] = sel.xpath('//tr[2]/td[2]/a/text()').extract()
            #print "pub, platform, cve, type : ", item['published'], item['platform'], item['cve'], item['_type']
            yield item